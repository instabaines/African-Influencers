{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>African Influencer for twitter marketing\n",
    "</big>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import the necessary libraries</h1>\n",
    "<p><b>Visit <a href='https://medium.com/@amureridwan002/african-influencers-twitter-users-segmentation-6eb3d149d298?source=friends_link&sk=43f7e398ca7bd877bdfa540d6520071d'>this medium post</a> for discussions</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import tweepy\n",
    "import jsonpickle\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import re\n",
    "import fire\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawler below fetch data from the internet, this is iur first source of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../pyscrap_url.py\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)\n",
    "    \n",
    "def get_elements(url, tag='h2',search={}, fname=None):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(url,str):\n",
    "        response = simple_get(url)\n",
    "    else:\n",
    "        #if already it is a loaded html page\n",
    "        response = url\n",
    "\n",
    "    if response is not None:\n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        \n",
    "        res = []\n",
    "        if tag:    \n",
    "            for li in html.select(tag):\n",
    "                for name in li.text.split('\\n'):\n",
    "                    if len(name) > 0:\n",
    "                        res.append(name.strip())\n",
    "                       \n",
    "                \n",
    "        if search:\n",
    "            soup = html            \n",
    "            \n",
    "            \n",
    "            r = ''\n",
    "            if 'find' in search.keys():\n",
    "                print('findaing',search['find'])\n",
    "                soup = soup.find(**search['find'])\n",
    "                r = soup\n",
    "\n",
    "                \n",
    "            if 'find_all' in search.keys():\n",
    "                print('findaing all of',search['find_all'])\n",
    "                r = soup.find_all(**search['find_all'])\n",
    "   \n",
    "            if r:\n",
    "                for x in list(r):\n",
    "                    if len(x) > 0:\n",
    "                        res.extend(x)\n",
    "            \n",
    "        return res\n",
    "\n",
    "    # Raise an exception if we failed to get any data from the url\n",
    "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
    "    \n",
    "    \n",
    "if get_ipython().__class__.__name__ == '__main__':\n",
    "    fire(get_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = get_elements('https://africafreak.com/100-most-influential-twitter-users-in-africa')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist_100_inf=[]\n",
    "for i in res:\n",
    "    if len(mylist_100_inf)<100:\n",
    "        mylist_100_inf.append(i.split('@')[-1].split(')')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist_100_df=pd.DataFrame(mylist_100_inf)\n",
    "#mylist.to_csv('10 influencers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
    "response = simple_get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_elements(response, search={'find_all':{'class_':'wp-block-embed__wrapper'}})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex ='#https?://twitter\\.com/(?:\\#!/)?(\\w+)/status(es)?/(\\d+)#is'\n",
    "a=re.match('((https?://)?(www\\.)?twitter\\.com/)(@|#!/)?([A-Za-z0-9_]{1,15})(/([-a-z]{1,20}))?','https://twitter.com/SE_Rajoelina/status/1241101811647500288')\n",
    "#print(a.group(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist=[]\n",
    "for i in res:\n",
    "    a=re.search('((https?://)?(www\\.)?twitter\\.com/)?(@|#!/)([A-Za-z0-9_]{1,15})(/([-a-z]{1,20}))?',str(i))\n",
    "    if a!=None:\n",
    "        mylist.append((a[0]))\n",
    "mylist[:10]\n",
    "#mylist=pd.DataFrame(mylist[:10])\n",
    "#mylist.to_csv('10 African leaders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = '#########'\n",
    "consumer_secret = '#########'\n",
    "access_token = '#########'\n",
    "access_token_secret = '#########'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##No of likes\n",
    "fav_count={i:'x' for i in mylist}\n",
    "No_of_followers={i :'x'for i in mylist}\n",
    "No_of_following={i :'x'for i in mylist}\n",
    "for i in mylist:\n",
    "        user = api.get_user(i)\n",
    "        fav_count[i]=user.favourites_count\n",
    "        No_of_followers[i]=user.followers_count\n",
    "        No_of_following[i]=user.friends_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tweet in tweepy.Cursor(api.user_timeline,id=mylist[0]).items():\n",
    "   # print (tweet)\n",
    "    #op=tweet._json\n",
    "#op['entities']['hashtags'][0]['text']\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Mentions\n",
    "mention_count={i:'x' for i in mylist}\n",
    "for i in my_list:\n",
    "    for results in tweepy.Cursor(twitter_api.search, q=i).items(200):\n",
    "        op=tweet._json  \n",
    "        count=+1\n",
    "        mention_count[i]=count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Twitter cawler use twitter API to fetch required information</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in mylist_100_inf:\n",
    "    try:\n",
    "        tweets = api.user_timeline(screen_name=j, \n",
    "                           # 200 is the maximum allowed count\n",
    "                           count=200,\n",
    "                           include_rts = False,\n",
    "                           # Necessary to keep full_text \n",
    "                           # otherwise only the first 140 words are extracted\n",
    "                           tweet_mode = 'extended')\n",
    "    except tweepy.TweepError:\n",
    "            continue\n",
    "    for tweet in tweets:\n",
    "            tweet_json=tweet._json\n",
    "            date=[]\n",
    "            tweet=[]\n",
    "            users_mention=[]\n",
    "            hashtag=[]\n",
    "            retweet_count=[]\n",
    "            retweeted=[]\n",
    "            location=[]\n",
    "            followers=[]\n",
    "            verified=[]\n",
    "            country=[]\n",
    "            fav_count=[]\n",
    "            user=[]\n",
    "            comment=[]\n",
    "            following=[]\n",
    "\n",
    "            \n",
    "            i=tweet_json\n",
    "            #print (j)\n",
    "            #country.append(i['place']['country'])\n",
    "            fav_count.append(i['favorite_count'])\n",
    "            user.append(j)\n",
    "            verified.append(i['user']['verified'])\n",
    "            followers.append(i['user']['followers_count'])\n",
    "            location.append(i['user']['location'])\n",
    "            date.append(i['created_at'])\n",
    "            tweet.append(i['full_text'])\n",
    "            try:\n",
    "                users_mention.append(i['entities']['user_mentions'][0]['screen_name'])\n",
    "            except IndexError as e:\n",
    "                users_mention.append('Nan')\n",
    "            try:\n",
    "                hashtag.append(i['entities']['hashtags'][0]['text'])\n",
    "            except IndexError as e:\n",
    "                hashtag.append('Nan')\n",
    "            retweet_count.append(i['retweet_count'])\n",
    "            retweeted.append(i['retweeted'])\n",
    "            if i['in_reply_to_status_id']:\n",
    "                comment.append(1)\n",
    "            else:\n",
    "                comment.append(0)\n",
    "            following.append(i['user']['friends_count'])\n",
    "            dict={'date_created':date,'user':user,'tweet':tweet,'user_mention':users_mention,'retweet_count':retweet_count\n",
    "                ,'retweeted':retweeted,'location':location,'followers':followers,'following':following,'verfied':verified,'hashtag':hashtag,'comment':comment,'likes':fav_count}\n",
    "            df=pd.DataFrame(dict)\n",
    "            try:\n",
    "                I_A=pd.concat([df,I_A])\n",
    "            except NameError:\n",
    "                I_A=df\n",
    "    \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "I_A.to_csv('Influencer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ireachscore=[]\n",
    "Ipopularityscore=[]\n",
    "for i in mylist_100_inf:\n",
    "    retweet=I_A.loc[I_A['user']==i, 'retweet_count'].sum()\n",
    "    Ipopularityscore.append(retweet)\n",
    "    try:\n",
    "        Ireachscore.append(I_A.loc[I_A['user']==i, 'followers'].unique().item()-I_A.loc[I_A['user']==i, 'following'].unique().item())\n",
    "    except ValueError  :\n",
    "        Ireachscore.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Areachscore=[]\n",
    "Apopularityscore=[]\n",
    "for i in mylist:\n",
    "    retweet=f_A.loc[f_A['user']==i, 'retweet_count'].sum()\n",
    "    Apopularityscore.append(retweet)\n",
    "    try:\n",
    "        Areachscore.append(f_A.loc[f_A['user']==i, 'followers'].unique().item()-f_A.loc[f_A['user']==i, 'following'].unique().item())\n",
    "    except ValueError  :\n",
    "        Areachscore.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influencer_score=pd.DataFrame({'Influencer':mylist_100_inf,'reach':Ireachscore,'Popularity':Ipopularityscore})\n",
    "Leaders_score=pd.DataFrame({'Influencer':mylist,'reach':Areachscore,'Popularity':Apopularityscore})\n",
    "influencer_score.to_csv('influencer_score.csv')\n",
    "Leaders_score.to_csv('Leaders_score')\n",
    "Leaders_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in mylist:\n",
    "    try:\n",
    "        tweets = api.user_timeline(screen_name=j, \n",
    "                           # 200 is the maximum allowed count\n",
    "                           count=200,\n",
    "                           include_rts = False,\n",
    "                           # Necessary to keep full_text \n",
    "                           # otherwise only the first 140 words are extracted\n",
    "                           tweet_mode = 'extended')\n",
    "    except tweepy.TweepError:\n",
    "            continue\n",
    "    for tweet in tweets:\n",
    "            tweet_json=tweet._json\n",
    "            date=[]\n",
    "            tweet=[]\n",
    "            users_mention=[]\n",
    "            hashtag=[]\n",
    "            retweet_count=[]\n",
    "            retweeted=[]\n",
    "            location=[]\n",
    "            followers=[]\n",
    "            verified=[]\n",
    "            country=[]\n",
    "            fav_count=[]\n",
    "            user=[]\n",
    "            comment=[]\n",
    "            following=[]\n",
    "\n",
    "            \n",
    "            i=tweet_json\n",
    "            #print (j)\n",
    "            #country.append(i['place']['country'])\n",
    "            fav_count.append(i['favorite_count'])\n",
    "            user.append(j)\n",
    "            verified.append(i['user']['verified'])\n",
    "            followers.append(i['user']['followers_count'])\n",
    "            location.append(i['user']['location'])\n",
    "            date.append(i['created_at'])\n",
    "            tweet.append(i['full_text'])\n",
    "            try:\n",
    "                users_mention.append(i['entities']['user_mentions'][0]['screen_name'])\n",
    "            except IndexError as e:\n",
    "                users_mention.append('Nan')\n",
    "            try:\n",
    "                hashtag.append(i['entities']['hashtags'][0]['text'])\n",
    "            except IndexError as e:\n",
    "                hashtag.append('Nan')\n",
    "            retweet_count.append(i['retweet_count'])\n",
    "            retweeted.append(i['retweeted'])\n",
    "            if i['in_reply_to_status_id']:\n",
    "                comment.append(1)\n",
    "            else:\n",
    "                comment.append(0)\n",
    "            following.append(i['user']['friends_count'])\n",
    "            dict={'date_created':date,'user':user,'tweet':tweet,'user_mention':users_mention,'retweet_count':retweet_count\n",
    "                ,'retweeted':retweeted,'location':location,'followers':followers,'following':following,'verfied':verified,'hashtag':hashtag,'comment':comment,'likes':fav_count}\n",
    "            df=pd.DataFrame(dict)\n",
    "            try:\n",
    "                f_A=pd.concat([df,f_A])\n",
    "            except NameError:\n",
    "                f_A=df\n",
    "    \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_A.to_csv('African Leaders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_A=pd.read_csv(r'C:\\Users\\Hp\\Downloads\\African Leaders.csv')\n",
    "#I_A=pd.read_csv(r'C:\\Users\\Hp\\Downloads\\Influencer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "newfile=pd.concat([f_A,I_A])\n",
    "a=Counter(newfile.hashtag)\n",
    "del a['Nan']\n",
    "a={k: v for k, v in sorted(a.items(),reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "a=a.items()\n",
    "a=[i[0] for i in a]\n",
    "record_I={i:'x' for i in a[:5]}\n",
    "for i in a[:5]:\n",
    "    record_I[i]=len(I_A.loc[I_A['hashtag']==i])\n",
    "record_A={i:'x' for i in a[:5]}\n",
    "for i in a[:5]:\n",
    "    record_A[i]=len(f_A.loc[f_A['hashtag']==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_I=list(record_I.values())\n",
    "record_A=list(record_A.values())\n",
    "sum_T=sum(record_I)+sum(record_A)\n",
    "record_I=[100*i/sum(record_I) for i in record_I]\n",
    "record_A=[100*i/sum(record_A) for i in record_A]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=a[:5]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35\n",
    "plt.style.use('seaborn')\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, record_I, width, label='Government')\n",
    "rects2 = ax.bar(x + width/2, record_A, width, label='Influencer')\n",
    "ax.set_ylabel('HashTags')\n",
    "ax.set_title('Fraction of influencers and gov officials by hashtag')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "#autolabel(rects1)\n",
    "#autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('final plot.jpg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
